{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Midterm Assignment (Make a Bipedal Robot Walk) <a class=\"tocSkip\">\n",
    "\n",
    "Its midterm season Wizards! The midterm is to make a bipedal humanoid robot walk in a simulation.\n",
    "You can use OpenAI Gym for the environment, this [link](https://github.com/search?q=bipedal+gym) shows some potential solutions that you can use to help you when you build your own. Submit your repository to schoolofaigrading@gmail.com . We will review your work and send back grades! Weâ€™re looking for good documentation, readable code, and bonus points for using reinforcement learning in a novel way for this challenge. Due date for all midterms is October 29, 2018 at 12 PM PST. Good luck!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#The-code\" data-toc-modified-id=\"The-code-1\">The code</a></span><ul class=\"toc-item\"><li><span><a href=\"#imports\" data-toc-modified-id=\"imports-1.1\">imports</a></span></li><li><span><a href=\"#slider-widget-code\" data-toc-modified-id=\"slider-widget-code-1.2\">slider widget code</a></span></li><li><span><a href=\"#Hyperparameters-Class\" data-toc-modified-id=\"Hyperparameters-Class-1.3\">Hyperparameters Class</a></span></li><li><span><a href=\"#ARS-agent-classes\" data-toc-modified-id=\"ARS-agent-classes-1.4\">ARS agent classes</a></span></li><li><span><a href=\"#helper-functions\" data-toc-modified-id=\"helper-functions-1.5\">helper functions</a></span></li><li><span><a href=\"#Hyperparameter-sliders\" data-toc-modified-id=\"Hyperparameter-sliders-1.6\">Hyperparameter sliders</a></span></li><li><span><a href=\"#Train-the-agent\" data-toc-modified-id=\"Train-the-agent-1.7\">Train the agent</a></span></li></ul></li><li><span><a href=\"#Experimental-Results\" data-toc-modified-id=\"Experimental-Results-2\">Experimental Results</a></span></li><li><span><a href=\"#Here-is-a-video-of-one-of-the-best-results-achieved-by-training\" data-toc-modified-id=\"Here-is-a-video-of-one-of-the-best-results-achieved-by-training-3\">Here is a video of one of the best results achieved by training</a></span><ul class=\"toc-item\"><li><span><a href=\"#Additional-Information\" data-toc-modified-id=\"Additional-Information-3.1\">Additional Information</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import gym\n",
    "import ipywidgets as widgets\n",
    "\n",
    "from gym import wrappers\n",
    "from tqdm import tnrange, tqdm_notebook\n",
    "from ipywidgets import interact, interactive\n",
    "from IPython.display import display\n",
    "\n",
    "ENV_NAME = 'BipedalWalker-v2'\n",
    "hyperparameters = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### slider widget code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Code to create hyperparameter widgets (sliders)\"\"\"\n",
    "seed = widgets.IntSlider(\n",
    "    description='seed:', value=1946, min=0, max=3000, orientation='vertical')\n",
    "num_episodes = widgets.IntSlider(\n",
    "    description='num_episodes:', value=200, min=0, max=10000, step=10, orientation='vertical')\n",
    "learning_rate = widgets.FloatSlider(\n",
    "    description='learning_rate:', value=0.2, min=0.0, max=1.0, orientation='vertical')\n",
    "record_every = widgets.IntSlider(\n",
    "    description='record_every:', value=50, min=0, max=1000, step=10, orientation='vertical')\n",
    "episode_length = widgets.IntSlider(\n",
    "    description='episode_length:', value=2000, min=0, max=10000, step=100, orientation='vertical')\n",
    "num_deltas = widgets.IntSlider(\n",
    "    description='num_deltas:', value=16, min=0, max=100, orientation='vertical')\n",
    "num_best_deltas = widgets.IntSlider(\n",
    "    description='num_best_deltas:', value=16, min=0, max=100, orientation='vertical')\n",
    "noise = widgets.FloatSlider(\n",
    "    description='noise:', value=0.03, min=0.0, max=1.0, step=0.01, orientation='vertical')\n",
    "\n",
    "ui_0 = widgets.HBox([\n",
    "    seed, num_episodes, learning_rate, record_every, episode_length, num_deltas, num_best_deltas, noise])\n",
    "\n",
    "def f_0(\n",
    "    seed, num_episodes, learning_rate, record_every, episode_length, num_deltas,\n",
    "    num_best_deltas, noise):\n",
    "    hyperparameters[\"seed\"] = seed\n",
    "    hyperparameters[\"num_episodes\"] = num_episodes\n",
    "    hyperparameters[\"learning_rate\"] = learning_rate  \n",
    "    hyperparameters[\"record_every\"] = record_every         \n",
    "    hyperparameters[\"episode_length\"] = episode_length        \n",
    "    hyperparameters[\"num_deltas\"] = num_deltas           \n",
    "    hyperparameters[\"num_best_deltas\"] = num_best_deltas\n",
    "    hyperparameters[\"noise\"] = noise   \n",
    "    \n",
    "out_0 = widgets.interactive_output(f_0, {\n",
    "    'seed': seed, 'num_episodes': num_episodes, \n",
    "    'learning_rate': learning_rate, 'record_every': record_every,\n",
    "    'episode_length': episode_length, 'num_deltas': num_deltas,\n",
    "    'num_best_deltas': num_best_deltas, 'noise': noise\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HyperParams():\n",
    "    \"\"\"Hyperparameters\"\"\"\n",
    "    def __init__(self,\n",
    "                 num_episodes=200,\n",
    "                 episode_length=2000,\n",
    "                 learning_rate=0.02,\n",
    "                 num_deltas=16,\n",
    "                 num_best_deltas=16,\n",
    "                 noise=0.03,\n",
    "                 seed=1,\n",
    "                 env_name=ENV_NAME,\n",
    "                 record_every=50):\n",
    "\n",
    "        self.num_episodes = num_episodes\n",
    "        self.episode_length = episode_length\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_deltas = num_deltas\n",
    "        self.num_best_deltas = num_best_deltas\n",
    "        assert self.num_best_deltas <= self.num_deltas\n",
    "        self.noise = noise\n",
    "        self.seed = seed\n",
    "        self.env_name = env_name\n",
    "        self.record_every = record_every"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ARS agent classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Normalizer():\n",
    "    # Normalizes the inputs\n",
    "    def __init__(self, nb_inputs):\n",
    "        self.n = np.zeros(nb_inputs)\n",
    "        self.mean = np.zeros(nb_inputs)\n",
    "        self.mean_diff = np.zeros(nb_inputs)\n",
    "        self.var = np.zeros(nb_inputs)\n",
    "\n",
    "    def observe(self, x):\n",
    "        self.n += 1.0\n",
    "        last_mean = self.mean.copy()\n",
    "        self.mean += (x - self.mean) / self.n\n",
    "        self.mean_diff += (x - last_mean) * (x - self.mean)\n",
    "        self.var = (self.mean_diff / self.n).clip(min = 1e-2)\n",
    "\n",
    "    def normalize(self, inputs):\n",
    "        obs_mean = self.mean\n",
    "        obs_std = np.sqrt(self.var)\n",
    "        return (inputs - obs_mean) / obs_std\n",
    "\n",
    "\n",
    "class Policy():\n",
    "    def __init__(self, input_size, output_size, hp, theta=None):\n",
    "        self.theta = theta or np.zeros((output_size, input_size))\n",
    "        self.hp = hp\n",
    "\n",
    "    def evaluate(self, state, delta=None, direction=None):\n",
    "        if direction is None:\n",
    "            return self.theta.dot(state)\n",
    "        elif direction == \"+\":\n",
    "            return (self.theta + self.hp.noise * delta).dot(state)\n",
    "        elif direction == \"-\":\n",
    "            return (self.theta - self.hp.noise * delta).dot(state)\n",
    "\n",
    "    def sample_deltas(self):\n",
    "        return [np.random.randn(*self.theta.shape) for _ in range(self.hp.num_deltas)]\n",
    "\n",
    "    def update(self, rollouts, sigma_rewards):\n",
    "        # sigma_rewards is the standard deviation of the rewards\n",
    "        step = np.zeros(self.theta.shape)\n",
    "        for r_pos, r_neg, delta in rollouts:\n",
    "            step += (r_pos - r_neg) * delta\n",
    "        self.theta += self.hp.learning_rate / (self.hp.num_best_deltas * sigma_rewards) * step\n",
    "\n",
    "    def get_theta(self):\n",
    "        return self.theta\n",
    "    \n",
    "    def set_theta(self, new_theta):\n",
    "        self.theta = new_theta\n",
    "                \n",
    "\n",
    "class ArsTrainer():\n",
    "    def __init__(self,\n",
    "                 hp=None,\n",
    "                 input_size=None,\n",
    "                 output_size=None,\n",
    "                 normalizer=None,\n",
    "                 policy=None,\n",
    "                 monitor_dir=None):\n",
    "\n",
    "        self.hp = hp or HyperParams()\n",
    "        np.random.seed(self.hp.seed)\n",
    "        self.env = gym.make(self.hp.env_name)\n",
    "        if monitor_dir is not None:\n",
    "            should_record = lambda i: self.record_video\n",
    "            self.env = wrappers.Monitor(self.env, monitor_dir, video_callable=should_record, resume=True)\n",
    "        self.hp.episode_length = self.env.spec.timestep_limit or self.hp.episode_length\n",
    "        self.input_size = input_size or self.env.observation_space.shape[0]\n",
    "        self.output_size = output_size or self.env.action_space.shape[0]\n",
    "        self.normalizer = normalizer or Normalizer(self.input_size)\n",
    "        self.policy = policy or Policy(self.input_size, self.output_size, self.hp)\n",
    "        self.record_video = False\n",
    "        \n",
    "    def __del__(self):\n",
    "        self.env.close()\n",
    "        \n",
    "    def save(self, filename='theta.npy'):\n",
    "        print(f\"Saving {filename}\") \n",
    "        np.save(filename, self.policy.get_theta())\n",
    "            \n",
    "    def load(self, filename='theta.npy'):\n",
    "        print(f\"Loading {filename}\")\n",
    "        self.policy.set_theta(np.load(filename))\n",
    "        \n",
    "    def get_policy(self):\n",
    "        return self.policy\n",
    "\n",
    "    \n",
    "    # Explore the policy on one specific direction and over one episode\n",
    "    def explore(self, direction=None, delta=None):\n",
    "        state = self.env.reset()\n",
    "        done = False\n",
    "        num_plays = 0.0\n",
    "        sum_rewards = 0.0\n",
    "        while not done and num_plays < self.hp.episode_length:\n",
    "            self.normalizer.observe(state)\n",
    "            state = self.normalizer.normalize(state)\n",
    "            action = self.policy.evaluate(state, delta=delta, direction=direction)\n",
    "            state, reward, done, _ = self.env.step(action)\n",
    "            reward = max(min(reward, 1), -1)\n",
    "            sum_rewards += reward\n",
    "            num_plays += 1\n",
    "        return sum_rewards\n",
    "\n",
    "    def play(self):\n",
    "        \"\"\"play stuff\"\"\"\n",
    "        self.record_video = True\n",
    "        self.explore()\n",
    "        \n",
    "    \n",
    "    def train(self):\n",
    "        \"\"\"trains the \"\"\"\n",
    "        best_reward, reward_evaluation = float(\"-inf\"), float(\"-inf\")\n",
    "        \n",
    "        # used by tqdm to give us a formatted progress bar\n",
    "        desc = f\"Current Reward: {reward_evaluation:>3.5f}, Best Reward: {reward_evaluation:>3.5f}\"\n",
    "        t = tnrange(self.hp.num_episodes, desc=desc, leave=True)\n",
    "        \n",
    "        rewards = np.zeros((self.hp.num_episodes, 1))\n",
    "        \n",
    "        for step in t:\n",
    "            #print(f\"Step: {step}\")\n",
    "            # used by tqdm to give us a formatted progress bar            \n",
    "            t.set_description(f\"Current Reward: {reward_evaluation:>3.5f}, Best Reward: {best_reward:>3.5f}\")\n",
    "            t.refresh()\n",
    "            \n",
    "            # initialize the random noise deltas and the positive/negative rewards\n",
    "            deltas = self.policy.sample_deltas()\n",
    "            positive_rewards = [0] * self.hp.num_deltas\n",
    "            negative_rewards = [0] * self.hp.num_deltas\n",
    "\n",
    "            # play an episode each with positive deltas and negative deltas, collect rewards\n",
    "            for k in range(self.hp.num_deltas):\n",
    "                positive_rewards[k] = self.explore(direction=\"+\", delta=deltas[k])\n",
    "                negative_rewards[k] = self.explore(direction=\"-\", delta=deltas[k])\n",
    "                \n",
    "            # Compute the standard deviation of all rewards\n",
    "            sigma_rewards = np.array(positive_rewards + negative_rewards).std()\n",
    "\n",
    "            # Sort the rollouts by the max(r_pos, r_neg) and select the deltas with best rewards\n",
    "            scores = {k:max(r_pos, r_neg) for k,(r_pos,r_neg) in enumerate(zip(positive_rewards, negative_rewards))}\n",
    "            order = sorted(scores.keys(), key = lambda x:scores[x], reverse = True)[:self.hp.num_best_deltas]\n",
    "            rollouts = [(positive_rewards[k], negative_rewards[k], deltas[k]) for k in order]\n",
    "\n",
    "            # Update the policy\n",
    "            self.policy.update(rollouts, sigma_rewards)\n",
    "\n",
    "            # Only record video during evaluation, every n steps\n",
    "            if step % self.hp.record_every == 0 and step > 0 or step == self.hp.num_episodes - 1:\n",
    "                self.record_video = True\n",
    "                \n",
    "            # Play an episode with the new weights and print the score\n",
    "            reward_evaluation = self.explore()\n",
    "            rewards[step] = reward_evaluation\n",
    "            \n",
    "            best_reward = max(best_reward, reward_evaluation)\n",
    "            self.record_video = False\n",
    "            \n",
    "            if step >= 10:\n",
    "                last_ten_mean = np.mean(rewards[-10:])\n",
    "                if last_ten_mean >= 300.00:\n",
    "                    print(f\"Finished training early, the last_ten_mean is {last_ten_mean} \"\n",
    "                          f\"after {step} steps\")\n",
    "            \n",
    "        return rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### helper functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mkdir(base, name):\n",
    "    path = os.path.join(base, name)\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    return path\n",
    "\n",
    "\n",
    "def train_agent(rewards_file='000.npy', continue_training=False):\n",
    "    \"\"\"Starts everything\"\"\"\n",
    "    print(hyperparameters)\n",
    "    videos_dir = mkdir('.', 'videos')\n",
    "    monitor_dir = mkdir(videos_dir, ENV_NAME)    \n",
    "    hp = HyperParams(**hyperparameters)\n",
    "    trainer = ArsTrainer(hp=hp, monitor_dir=monitor_dir)\n",
    "        \n",
    "    rewards = trainer.train()\n",
    "\n",
    "    rewards_dir = mkdir('.', 'rewards')\n",
    "    np.save(rewards_dir + \"/\" + rewards_file, rewards)\n",
    "\n",
    "    \n",
    "def play_agent(filename='theta.npy'):\n",
    "    videos_dir = mkdir('.', 'videos')\n",
    "    monitor_dir = mkdir(videos_dir, ENV_NAME)    \n",
    "    hp = HyperParams(**hyperparameters)\n",
    "    trainer = ArsTrainer(hp=hp, monitor_dir=monitor_dir)\n",
    "    trainer.load(filename=filename)\n",
    "    policy = trainer.get_policy()\n",
    "    print(f\"Policy.theta:\\n{policy.theta}\")\n",
    "    trainer.play()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter sliders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b514d78bd61b4f36bb2363dcdadb48bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntSlider(value=1946, description='seed:', max=3000, orientation='vertical'), IntSlider(value=1â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0b1881b59b74749a8217b4fa16455f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(ui_0, out_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'seed': 1946, 'num_episodes': 10, 'learning_rate': 0.2, 'record_every': 50, 'episode_length': 2000, 'num_deltas': 16, 'num_best_deltas': 16, 'noise': 0.03}\n"
     ]
    }
   ],
   "source": [
    "# Let's just verify our hyperparameters\n",
    "print(hyperparameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'seed': 1946, 'num_episodes': 10, 'learning_rate': 0.2, 'record_every': 50, 'episode_length': 2000, 'num_deltas': 16, 'num_best_deltas': 16, 'noise': 0.03}\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "082e924d876245d19cf645c94e82b853",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Current Reward: -inf, Best Reward: -inf', max=10, style=Progrâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_agent(rewards_file='experiment_14.npy', continue_training=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimental Results\n",
    "\n",
    "After 12 experiments, we found that **experiment 8** showed a pretty reasonable result within 200 episodes\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Experiment | Seed   | num_episodes | learning_rate | episode_length | noise  | best_reward |\n",
    "|------------|--------|--------------|---------------|----------------|--------|-------------|\n",
    "|           1|1946    | 200          |            0.2|            2000|   0.03 |         4.08|\n",
    "|           2|1946    | 200          |            0.2|            2000|   0.00 |        -1.25|\n",
    "|           3|1946    | 200          |            0.2|            2000|   0.90 |         9.63|\n",
    "|           4|1946    | 200          |            0.2|            2000|   0.90 |       263.60|\n",
    "|           5|1946    | 200          |            0.2|            2000|   0.90 |       263.57|\n",
    "|           6|1946    | 200          |            0.2|            2000|   0.50 |         2.57|         \n",
    "|           7|1946    | 200          |            0.2|            2000|   0.40 |         3.17|         \n",
    "|       **8**|**1946**| **200**      |        **0.2**|        **2000**|**0.25**|   **281.37**|         \n",
    "|           9|1946    | 200          |            0.2|            2000|    0.22|       248.47|\n",
    "|          10|1946    | 200          |            0.3|            2000|    0.22|       190.21|\n",
    "|          11|1946    | 300          |            0.3|            2000|    0.25|       276.58|\n",
    "|          12|1946    | 300          |            0.3|            2000|    0.10|        38.40|\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here is a video of one of the best results achieved by training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<video controls src=\"./videos/BipedalWalker-v2/openaigym.video.9.22269.video004982.mp4\" />"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:move_37]",
   "language": "python",
   "name": "conda-env-move_37-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
