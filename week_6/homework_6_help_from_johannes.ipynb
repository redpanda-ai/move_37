{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework Assignment 6: Deep Q Learning <a class=\"tocSkip\">\n",
    "\n",
    "This weeks homework assignment is build a deep q network to defeat the [Lunar Lander](https://gym.openai.com/envs/LunarLander-v2/) environment in OpenAI’s Gym environment. Use this [repository](https://github.com/AndersonJo/dqn-pytorch) as a helpful guide. Train it and test it, if your algorithm successfully learns how to beat the environment, you’ve successfully completed the assignment. Good luck!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Exercise\" data-toc-modified-id=\"Exercise-1\">Exercise</a></span><ul class=\"toc-item\"><li><span><a href=\"#Global-Parameters\" data-toc-modified-id=\"Global-Parameters-1.1\">Global Parameters</a></span></li><li><span><a href=\"#ReplayMemory-class\" data-toc-modified-id=\"ReplayMemory-class-1.2\">ReplayMemory class</a></span></li><li><span><a href=\"#DQN-class,-our-Deep-Q-Network-class\" data-toc-modified-id=\"DQN-class,-our-Deep-Q-Network-class-1.3\">DQN class, our Deep Q Network class</a></span><ul class=\"toc-item\"><li><span><a href=\"#Network-design\" data-toc-modified-id=\"Network-design-1.3.1\">Network design</a></span></li></ul></li><li><span><a href=\"#LSTMDQN-class,-a-Deep-Q-Network-class-that-uses-LSTM-memory\" data-toc-modified-id=\"LSTMDQN-class,-a-Deep-Q-Network-class-that-uses-LSTM-memory-1.4\">LSTMDQN class, a Deep Q Network class that uses LSTM memory</a></span><ul class=\"toc-item\"><li><span><a href=\"#Network-design\" data-toc-modified-id=\"Network-design-1.4.1\">Network design</a></span></li></ul></li><li><span><a href=\"#Environment-class,-a-gym-game\" data-toc-modified-id=\"Environment-class,-a-gym-game-1.5\">Environment class, a gym game</a></span></li><li><span><a href=\"#Agent-Class\" data-toc-modified-id=\"Agent-Class-1.6\">Agent Class</a></span></li><li><span><a href=\"#Main-function\" data-toc-modified-id=\"Main-function-1.7\">Main function</a></span></li><li><span><a href=\"#Arguments\" data-toc-modified-id=\"Arguments-1.8\">Arguments</a></span></li></ul></li><li><span><a href=\"#Research\" data-toc-modified-id=\"Research-2\">Research</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Voice-search-my-browsing-history\" data-toc-modified-id=\"Voice-search-my-browsing-history-2.0.1\">Voice search my browsing history</a></span></li></ul></li></ul></li><li><span><a href=\"#Big-Questions\" data-toc-modified-id=\"Big-Questions-3\">Big Questions</a></span></li><li><span><a href=\"#Resources\" data-toc-modified-id=\"Resources-4\">Resources</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import argparse\n",
    "import copy\n",
    "import cv2\n",
    "import glob\n",
    "import gym\n",
    "import logging\n",
    "import math\n",
    "import os\n",
    "import pylab\n",
    "import re\n",
    "import sys\n",
    "import torch\n",
    "\n",
    "# import gym_ple\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "\n",
    "from collections import deque, namedtuple\n",
    "from gym.wrappers import Monitor\n",
    "from random import random, sample\n",
    "from scipy.misc import toimage\n",
    "from torch import nn, optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from torch.nn import functional as F\n",
    "from torchvision import transforms as T\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "USE_CPU = False # can be True or False\n",
    "GPU = 1 # can be 0 or 1\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "# Replay Memory\n",
    "REPLAY_MEMORY = 500\n",
    "\n",
    "# Epsilon\n",
    "EPSILON_START = 1.0\n",
    "EPSILON_END = 0.01\n",
    "EPSILON_DECAY = 5000\n",
    "\n",
    "# LSTM Memory\n",
    "LSTM_MEMORY = 128\n",
    "\n",
    "# ETC Options\n",
    "TARGET_UPDATE_INTERVAL = 100\n",
    "CHECKPOINT_INTERVAL = 500\n",
    "PLAY_INTERVAL = 90 # 90\n",
    "PLAY_REPEAT = 1000 # 1000\n",
    "LEARNING_RATE = 0.01\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ReplayMemory class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory(object):\n",
    "    \n",
    "    def __init__(self, capacity=REPLAY_MEMORY):\n",
    "        self.capacity = capacity\n",
    "        self.memory = deque(maxlen=self.capacity)\n",
    "        self.Transition = namedtuple('Transition', ('state', 'action', 'reward', 'next_state'))\n",
    "        self._available = False\n",
    "        \n",
    "    def put(self, state: np.array, action: torch.LongTensor, reward: np.array, next_state: np.array):\n",
    "        \"\"\"doc-string goes here\"\"\"\n",
    "        state = torch.FloatTensor(state)\n",
    "        reward = torch.FloatTensor([reward])\n",
    "        if next_state is not None:\n",
    "            next_state = torch.FloatTensor(next_state)\n",
    "        transition = self.Transition(state=state, action=action, reward=reward, next_state=next_state)\n",
    "        self.memory.append(transition)\n",
    "        \n",
    "    def sample(self, batch_size):\n",
    "        transitions = sample(self.memory, batch_size)\n",
    "        return self.Transition(*(zip(*transitions))) # this notation deserves study\n",
    "        \n",
    "    def size(self):\n",
    "        return len(self.memory)\n",
    "    \n",
    "    def is_available(self):\n",
    "        if self._available:\n",
    "            return True\n",
    "        \n",
    "        if len(self.memory) > BATCH_SIZE:\n",
    "            self._available = True\n",
    "        return self._available\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN class, our Deep Q Network class\n",
    "\n",
    "### Network design\n",
    "\n",
    "1. the data within the model will move through the layers\n",
    "2. the \"forward\" method will pull our 'x' through the layers and then apply ReLU functions to keep our values positive\n",
    "3. $ input (naction) -> conv1(4, 32) -> ReLU -> conv2(32, 64) -> ReLU -> conv3(64, 64) -> ReLU -> affine1(3136, 512) -> affine2(h) -> output $\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, n_action):\n",
    "        super(DQN, self).__init__()\n",
    "        self.n_action = n_action\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(4, 32, kernel_size=8, stride=4, padding=0) # (In Channel, Out Channel, ...)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=0)\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=0)\n",
    "\n",
    "        self.affine1 = nn.Linear(3136, 512)\n",
    "        self.affine2 = nn.Linear(512, self.n_action)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = F.relu(self.conv1(x))\n",
    "        h = F.relu(self.conv2(h))\n",
    "        h = F.relu(self.conv3(h))\n",
    "        \n",
    "        h = F.relu(self.affine1(h.view(h.size(0), -1)))\n",
    "        h = self.affine2(h)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTMDQN class, a Deep Q Network class that uses LSTM memory\n",
    "\n",
    "### Network design\n",
    "\n",
    "1. the data within the model will move through the layers\n",
    "2. the \"forward\" method will pull our 'x' through the layers and then apply ReLU functions to keep our values positive\n",
    "3. $ input (naction) -> conv1(4, 32) -> ReLU -> conv2(32, 64) -> ReLU -> conv3(64, 64) -> ReLU -> affine1(3136, 512) -> affine2(h) -> output $\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMDQN(nn.Module):\n",
    "    def __init__(self, n_action):\n",
    "        super(LSTMDQN, self).__init__()\n",
    "        self.n_action = n_action\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(4, 32, kernel_size=8, stride=1, padding=1)  # (In Channel, Out Channel, ...)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=5, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv4 = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "        self.lstm = nn.LSTM(16, LSTM_MEMORY, 1)  # (Input, Hidden, Num Layers)\n",
    "        \n",
    "        self.affine1 = nn.Linear(LSTM_MEMORY * 64, 512)\n",
    "        self.affine2 = nn.Linear(512, self.n_action)\n",
    "        \n",
    "    def forward(self, x, hidden_state, cell_state):\n",
    "        # CNN\n",
    "        h = F.relu(F.max_pool2d(self.conv1(x), kernel_size=2, stride=2))\n",
    "        h = F.relu(F.max_pool2d(self.conv2(x), kernel_size=2, stride=2))\n",
    "        h = F.relu(F.max_pool2d(self.conv3(x), kernel_size=2, stride=2))\n",
    "        h = F.relu(F.max_pool2d(self.conv4(x), kernel_size=2, stride=2))\n",
    "        \n",
    "        # LSTM\n",
    "        h = h.view(h.size(0), h.size(1), 16)  # (32, 64, 4, 4) -> (32, 64, 16)\n",
    "        h, (next_hidden_state, next_cell_state) = self.lstm(h, (hidden_state, cell_state))\n",
    "        h = h.view(h.size(0), -1) # (32, 64, 256) -> (32, 64 * 256)\n",
    "        \n",
    "        # Fully Connected Layers\n",
    "        h = F.relu(self.affine(h.view(h.size(0) -1)))\n",
    "        h = self.affine2(h)\n",
    "        \n",
    "    def init_states(self) -> [Variable, Variable]:\n",
    "        hidden_state = Variable(torch.zeros(1, 64, LSTM_MEMORY).cuda())\n",
    "        cell_state = Variable(torch.zeros(1, 64, LSTM_MEMORY).cuda())\n",
    "        return hidden_state.detach(), cell_state.detach()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment class, a gym game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment(object):\n",
    "    def __init__(self, game, record=False, width=84, height=84, seed=0):\n",
    "        self.game = gym.make(game)\n",
    "        self.game.seed(seed)\n",
    "        \n",
    "        if record:\n",
    "            self.game = Monitor(self.game, './video', force=True)\n",
    "            \n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self._toTensor = T.Compose([T.ToPILImage(), T.ToTensor()])\n",
    "        # gym_ple\n",
    "        \n",
    "    def play_simple(self, mode: str = 'human'):\n",
    "        observation = self.game.reset()\n",
    "        \n",
    "        while True:\n",
    "            screen = self.game.render(mode=mode)\n",
    "            if mode == 'rgb_array':\n",
    "                screen = self.preprocess(screen)\n",
    "            action = self.game.action_space.sample()\n",
    "            observation, reward, done, info = self.game.step(action)\n",
    "            if done:\n",
    "                break\n",
    "        self.game.close()\n",
    "        \n",
    "    def preprocess(self, screen):\n",
    "        preprocessed: np.array = cv2.resize(screen, (self.height, self.width)) # 84 * 84\n",
    "        preprocessed = np.dot(preprocessed[..., :3], [0.299, 0.587, 0.114]) # Gray scale\n",
    "        preprocessed: np.array = preprocessed.astype('float32') / 255.\n",
    "\n",
    "        return preprocessed\n",
    "\n",
    "    def init(self):\n",
    "        \"\"\"\n",
    "        @return observation\n",
    "        \"\"\"\n",
    "        return self.game.reset()\n",
    "\n",
    "    def get_screen(self):\n",
    "        screen = self.game.render('rgb_array')\n",
    "        screen = self.preprocess(screen)\n",
    "        return screen\n",
    "\n",
    "    def step(self, action: int):\n",
    "        observation, reward, done, info = self.game.step(action)\n",
    "        return observation, reward, done, info\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"doc-string\"\"\"\n",
    "        observation = self.game.reset()\n",
    "        observation = self.preprocess(observation)\n",
    "        return observation\n",
    "\n",
    "    @property\n",
    "    def action_space(self):\n",
    "        return self.game.action_space.n\n",
    "        \n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(object):\n",
    "    def __init__(self, cuda=True, action_repeat: int = 4):\n",
    "        \"\"\"doc-string here\"\"\"\n",
    "        self.clip: bool = CLIP\n",
    "        self.seed: int = SEED\n",
    "        self.action_repeat: int = action_repeat\n",
    "        self.frame_skipping: int = SKIP_ACTION\n",
    "        self._state_buffer = deque(maxlen=self.action_repeat)\n",
    "        self.step = 0\n",
    "        self.best_score = BEST or -10000\n",
    "        self.best_count = 0\n",
    "\n",
    "        self._play_steps = deque(maxlen=5)\n",
    "\n",
    "        # Environment\n",
    "        self.env = Environment(GAME, record=RECORD, seed=SEED)\n",
    "\n",
    "        # DQN Model\n",
    "        self.dqn_hidden_state = self.dqn_cell_state = None\n",
    "        self.target_hidden_state = self.target_cell_state = None\n",
    "\n",
    "        self.mode: str = MODEL.lower()\n",
    "        if self.mode == 'dqn':\n",
    "            self.dqn: DQN = DQN(self.env.action_space)\n",
    "        elif self.mode == 'lstm':\n",
    "            self.dqn: LSTMDQN = LSTMDQN(self.env.action_space)\n",
    "\n",
    "            # For Optimization\n",
    "            self.dqn_hidden_state, self.dqn_cell_state = self.dqn.init_states()\n",
    "            self.target_hidden_state, self.target_cell_state = self.dqn.init_states()\n",
    "\n",
    "            # For Training Play\n",
    "            self.train_hidden_state, self.train_cell_state = self.dqn.init_states()\n",
    "\n",
    "            # For Validation Play\n",
    "            self.test_hidden_state, self.test_cell_state = self.dqn.init_states()\n",
    "\n",
    "        if cuda:\n",
    "            self.dqn.cuda()\n",
    "\n",
    "        # DQN Target Model\n",
    "        self.target: DQN = copy.deepcopy(self.dqn)\n",
    "\n",
    "        # Optimizer\n",
    "        self.optimizer = optim.Adam(self.dqn.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "        # Replay Memory\n",
    "        self.replay = ReplayMemory()\n",
    "\n",
    "        # Epsilon\n",
    "        self.epsilon = EPSILON_START\n",
    "\n",
    "    def select_action(self, states: np.array) -> tuple:\n",
    "        \"\"\"doc-string here\"\"\"\n",
    "        # Decrease epsilon value\n",
    "        self.epsilon = EPSILON_END + (EPSILON_START - EPSILON_END) * \\\n",
    "            math.exp(-1. * self.step / EPSILON_DECAY)\n",
    "\n",
    "        if self.epsilon > random():\n",
    "            # Random Action\n",
    "            sample_action = self.env.game.action_space.sample()\n",
    "            action = torch.LongTensor([[sample_action]])\n",
    "            return action\n",
    "\n",
    "        states = states.reshape(1, self.action_repeat, self.env.width, self.env.height)\n",
    "        states_variable: Variable = Variable(torch.FloatTensor(states).cuda())\n",
    "\n",
    "        if self.mode == 'dqn':\n",
    "            states_variable.volatile = True\n",
    "            action = self.dqn(states_variable).data.cpu().max(1)[1].unsqueeze(1)\n",
    "        elif self.mode == 'lstm':\n",
    "            action, self.dqn_hidden_state, self.dqn_cell_state = \\\n",
    "                self.dqn(states_variable, self.train_hidden_state, self.train_cell_state)\n",
    "            action = action.data.cpu().max(1)[1]\n",
    "\n",
    "        return action\n",
    "\n",
    "    def get_initial_states(self):\n",
    "        state = self.env.reset()\n",
    "        state = self.env.get_screen()\n",
    "        states = np.stack([state for _ in range(self.action_repeat)], axis=0)\n",
    "\n",
    "        self._state_buffer = deque(maxlen=self.action_repeat)\n",
    "        for _ in range(self.action_repeat):\n",
    "            self._state_buffer.append(state)\n",
    "        return states\n",
    "\n",
    "    def add_state(self, state):\n",
    "        self._state_buffer.append(state)\n",
    "\n",
    "    def recent_states(self):\n",
    "        return np.array(self._state_buffer)\n",
    "\n",
    "    def train(self, gamma: float = 0.99, mode: str = 'rgb_array'):\n",
    "        # Initial States\n",
    "        reward_sum = 0.\n",
    "        q_mean = [0., 0.]\n",
    "        target_mean = [0., 0.]\n",
    "\n",
    "        while True:\n",
    "            # Init LSTM States\n",
    "            if self.mode == 'lstm':\n",
    "                # For Training\n",
    "                self.train_hidden_state, self.train_cell_state = \\\n",
    "                    self.dqn.reset_states(self.train_hidden_state, self.train_cell_state)\n",
    "\n",
    "            states = self.get_initial_states()\n",
    "            losses = []\n",
    "            checkpoint_flag = False\n",
    "            target_update_flag = False\n",
    "            play_flag = False\n",
    "            play_steps = 0\n",
    "            real_play_count = 0\n",
    "            real_score = 0\n",
    "\n",
    "            reward = 0\n",
    "            done = False\n",
    "            while True:\n",
    "                # Get Action\n",
    "                action: torch.LongTensor = self.select_action(states)\n",
    "\n",
    "                for _ in range(self.frame_skipping):\n",
    "                    observation, reward, done, info = self.env.step(int(action))\n",
    "                    next_state = self.env.get_screen()\n",
    "                    self.add_state(next_state)\n",
    "\n",
    "                    if done:\n",
    "                        break\n",
    "\n",
    "                # Store the information in Replay Memory\n",
    "                next_states = self.recent_states()\n",
    "                if done:\n",
    "                    self.replay.put(states, action, reward, None)\n",
    "                else:\n",
    "                    self.replay.put(states, action, reward, next_states)\n",
    "\n",
    "                # Change States\n",
    "                states = next_states\n",
    "\n",
    "                # Optimize\n",
    "                if self.replay.is_available():\n",
    "                    loss, reward_sum, q_mean, target_mean = self.optimize(gamma)\n",
    "                    losses.append(loss)\n",
    "\n",
    "                if done:\n",
    "                    break\n",
    "\n",
    "                # Increase step\n",
    "                self.step += 1\n",
    "                play_steps += 1\n",
    "\n",
    "                # Target Update\n",
    "                if self.step % TARGET_UPDATE_INTERVAL == 0:\n",
    "                    self._target_update()\n",
    "                    target_update_flag = True\n",
    "\n",
    "                # Play\n",
    "                if self.step % PLAY_INTERVAL == 0:\n",
    "                    play_flag = True\n",
    "\n",
    "                    scores = []\n",
    "                    counts = []\n",
    "                    for _ in range(PLAY_REPEAT):\n",
    "                        score, real_play_count = self.play(logging=False, human=False)\n",
    "                        scores.append(score)\n",
    "                        counts.append(real_play_count)\n",
    "                        logger.debug(f\"[{self.step}] [Validation] play_score: {score}, \"\n",
    "                                     f\"play_count: {real_play_count}\")\n",
    "\n",
    "                    real_score = int(np.mean(scores))\n",
    "                    real_play_count = int(np.mean(counts))\n",
    "\n",
    "                    if self.best_score <= real_score:\n",
    "                        self.best_score = real_score\n",
    "                        self.best_count = real_play_count\n",
    "                        logger.debug(f\"[{self.step}] [Checkpoint] Play: {self.best_score} \"\n",
    "                                     f\"[Best Play] [checkpoint]\")\n",
    "                        self.save_checkpoint(\n",
    "                            filename=f\"dqn_checkpoints/chkpoint_{self.mode}_{self.best_score}.pth.tar\")\n",
    "\n",
    "            self._play_steps.append(play_steps)\n",
    "\n",
    "            # Play\n",
    "            if play_flag:\n",
    "                play_flag = False\n",
    "                logger.info(f\"[{self.step}] [Validation] mean_score: {real_score}, mean_play_count: \"\n",
    "                            f\"{real_play_count}\")\n",
    "\n",
    "            # Logging\n",
    "            mean_loss = np.mean(losses)\n",
    "            target_update_msg = '  [target updated]' if target_update_flag else ''\n",
    "\n",
    "    def optimize(self, gamma: float):\n",
    "        if self.mode == \"lstm\":\n",
    "            # For Optimization\n",
    "            self.dqn_hidden_state, self.dqn_cell_state = self.dqn.reset_states(\n",
    "                self.dqn_hidden_state, self.dqn_cell_state)\n",
    "            self.target_hidden_state, self.target_cell_state = self.dqn.reset_states(\n",
    "                self.target_hidden_state, self.target_cell_state)\n",
    "\n",
    "        # Get Sample\n",
    "        transitions = self.replay.sample(BATCH_SIZE)\n",
    "\n",
    "        # Mask\n",
    "        non_final_mask = \\\n",
    "            torch.ByteTensor(list(map(lambda ns: ns is not None, transitions.next_state))).cuda()\n",
    "\n",
    "        final_mask = 1 - non_final_mask\n",
    "\n",
    "        state_batch: Variable = Variable(torch.cat(transitions.state).cuda())\n",
    "        action_batch: Variable = Variable(torch.cat(transitions.action).cuda())\n",
    "        reward_batch: Variable = Variable(torch.cat(transitions.reward).cuda())\n",
    "\n",
    "        next_state_list = [ns for ns in transitions.next_state if ns is not None]\n",
    "        non_final_next_state_batch = Variable(torch.cat(next_state_list).cuda())\n",
    "        non_final_next_state_batch.volatile = True\n",
    "\n",
    "        # Reshape States and Next States\n",
    "        state_batch = state_batch.view([BATCH_SIZE, self.action_repeat, self.env.width, self.env.height])\n",
    "        non_final_next_state_batch = non_final_next_state_batch.view(\n",
    "            [-1, self.action_repeat, self.env.width, self.env.height])\n",
    "        non_final_next_state_batch.volatile = True\n",
    "        \n",
    "        # Predict by DQN Model\n",
    "        if self.mode == 'dqn':\n",
    "            q_pred = self.dqn(state_batch)\n",
    "        elif self.mode == 'lstm':\n",
    "            q_pred, self.dqn_hidden_state, self.dqn_cell_state = \\\n",
    "                self.dqn(state_batch, self.dqn_hidden_state, self.dqn_cell_state)\n",
    "\n",
    "        q_values = q_pred.gather(1, action_batch)\n",
    "\n",
    "        # Predict by Target Model\n",
    "        target_values = Variable(torch.zeros(BATCH_SIZE, 1).cuda())\n",
    "        if self.mode == 'dqn':\n",
    "            target_pred = self.target(non_final_next_state_batch)\n",
    "        elif self.mode == 'lstm':\n",
    "            target_pred, self.target_hidden_state, self.target_cell_state = \\\n",
    "                self.target(non_final_next_state_batch, self.target_hidden_state, self.target_cell_state)\n",
    "\n",
    "        test = reward_batch[non_final_mask] + target_pred.max(1)[0] * gamma\n",
    "        test = test.unsqueeze(1)\n",
    "\n",
    "        target_values[non_final_mask] = test\n",
    "        test2 = reward_batch[final_mask]\n",
    "\n",
    "        if test2.size()[0] != 0:\n",
    "            test2 = test2.unsqueeze(1)\n",
    "\n",
    "        target_values[final_mask] = test2\n",
    "\n",
    "        loss = F.smooth_l1_loss(q_values, target_values.detach())\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        if self.clip:\n",
    "            for param in self.dqn.parameters():\n",
    "                param.grad.data.clamp_(-1, 1)\n",
    "\n",
    "        self.optimizer.step()\n",
    "\n",
    "        reward_score = int(torch.sum(reward_batch).data.cpu().numpy())\n",
    "        q_mean = torch.sum(q_pred, 0).data.cpu().numpy()[0]\n",
    "        target_mean = torch.sum(target_pred, 0).data.cpu().numpy()[0]\n",
    "\n",
    "        return loss.data.cpu().numpy(), reward_score, q_mean, target_mean\n",
    "\n",
    "    def _target_update(self):\n",
    "        self.target = copy.deepcopy(self.dqn)\n",
    "\n",
    "    def save_checkpoint(self, filename='dqn/checkpoints/checkpoint.pth.tar'):\n",
    "        dirpath = os.path.dirname(filename)\n",
    "\n",
    "        if not os.path.exists(dirpath):\n",
    "            os.mkdir(dirpath)\n",
    "\n",
    "        checkpoint = {\n",
    "            'dqn': self.dqn.state_dict(),\n",
    "            'target': self.target.state_dict(),\n",
    "            'optimizer': self.optimizer.state_dict(),\n",
    "            'step': self.step,\n",
    "            'best': self.best_score,\n",
    "            'best_count': self.best_count\n",
    "        }\n",
    "        torch.save(checkpoint, filename)\n",
    "\n",
    "    def load_checkpoint(self, filename='dqn_checkpoints/checkpoint.pth.tar', epsilon=None):\n",
    "        checkpoint = torch.load(filename)\n",
    "        self.dqn.load_state_dict(checkpoint['dqn'])\n",
    "        self.target.load_state_dict(checkpoint['target'])\n",
    "        self.optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "        self.step = checkpoint['step']\n",
    "        self.best_score = self.best_score or checkpoint['best']\n",
    "        self.best_count = checkpoint['best_count']\n",
    "\n",
    "    def load_latest_checkpoint(self, epsilon=None):\n",
    "        r = re.compile('chkpoint_(dqn|lstm)_(?P<number>-?\\d+)\\.pth\\.tar$')\n",
    "\n",
    "        files = glob.glob(f'dqn_checkpoints/chkpoint_{self.mode}_*.pth.tar')\n",
    "\n",
    "        if files:\n",
    "            files = list(map(lambda x: [int(r.search(x).group('number')), x], files))\n",
    "            files = sorted(files, key=lambda x: x[0])\n",
    "            latest_file = files[-1][1]\n",
    "            self.load_checkpoint(latest_file, epsilon=epsilon)\n",
    "            print(f'latest checkpoint has been lodaed - {latest_file}')\n",
    "        else:\n",
    "            print('no latest checkpoint')\n",
    "\n",
    "    def play(self, logging=True, human=True):\n",
    "        self.env.game.close()\n",
    "        observation = self.env.game.reset()\n",
    "        states = self.get_initial_states()\n",
    "        count = 0\n",
    "        total_score = 0\n",
    "\n",
    "        self.env.game.seed(self.seed)\n",
    "\n",
    "        if self.mode == 'lstm':\n",
    "            self.test_hidden_state, self.test_cell_state = \\\n",
    "                self.dqn.reset_states(self.test_hidden_state, self.test_cell_state)\n",
    "\n",
    "        while True:\n",
    "            states = states.reshape(1, self.action_repeat, self.env.width, self.env.height)\n",
    "            states_variable: Variable = Variable(torch.FloatTensor(states).cuda())\n",
    "\n",
    "            if self.mode == 'dqn':\n",
    "                dqn_pred = self.dqn(states_variable)\n",
    "            elif self.mode == 'lstm':\n",
    "                dqn_pred, self.dqn_hidden_state, self.dqn_cell_state = \\\n",
    "                    self.dqn(states_variable, self.test_hidden_state, self.test_cell_state)\n",
    "\n",
    "            action = dqn_pred.data.cpu().max(1)[1][0]\n",
    "            action = int(action)\n",
    "\n",
    "            for _ in range(self.frame_skipping):\n",
    "                if human:\n",
    "                    screen = self.env.game.render(mode='human')\n",
    "                observation, reward, done, info = self.env.step(action)\n",
    "                # States <- Next States\n",
    "                next_state = self.env.get_screen()\n",
    "                self.add_state(next_state)\n",
    "                states = self.recent_states()\n",
    "\n",
    "                total_score += reward\n",
    "\n",
    "                if done:\n",
    "                    break\n",
    "\n",
    "            # Logging\n",
    "            count += 1\n",
    "            if logging:\n",
    "                action_dist = torch.sum(dqn_pred, 0).data.cpu().numpy()[0]\n",
    "                print(f'[{count}] action:{action} {action_dist}, reward:{reward}')\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        self.env.game.close()\n",
    "        return total_score, count\n",
    "\n",
    "    def inspect(self):\n",
    "        print(dir(self.dqn.conv1))\n",
    "\n",
    "        for param in list(self.dqn.parameters()):\n",
    "            print(param.size())\n",
    "\n",
    "        print(self.dqn.conv2.kernel_size)\n",
    "        print(self.dqn.conv3.kernel_size)\n",
    "        print(self.dqn.conv4.kernel_size)\n",
    "        print(self.dqn.conv5.kernel_size)\n",
    "\n",
    "    @property\n",
    "    def play_step(self):\n",
    "        return np.nan_to_num(np.mean(self._play_steps))\n",
    "\n",
    "    def _sum_params(self, model):\n",
    "        return np.sum([torch.sum(p).data[0] for p in model.parameters()])\n",
    "\n",
    "    def imshow(self, sample_image: np.array, transpose=False):\n",
    "        if transpose:\n",
    "            sample_image = sample_image.transpose((1, 2, 0))\n",
    "\n",
    "        pylab.imshow(sample_image, cmap='gray')\n",
    "        pylab.show()\n",
    "\n",
    "    def toimage(self, image: np.array, name: str):\n",
    "        toimage(image, cmin=0, cmax=255).save(name)\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    agent = Agent()\n",
    "    if LOAD_LATEST and not CHECKPOINT:\n",
    "        agent.load_latest_checkpoint()\n",
    "    elif CHECKPOINT:\n",
    "        agent.load_checkpoint(CHECKPOINT)\n",
    "        \n",
    "    if MODE == 'play':\n",
    "        agent.play()\n",
    "    elif MODE == 'train':\n",
    "        agent.train()\n",
    "    elif MODE == 'inspect':\n",
    "        agent.inspect()\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = \"dqn\"        # type=str, default='dqn', help='set model type'\n",
    "STEP = None          # type=int, default=None, help='forcefully set step'\n",
    "BEST = None          # type=int, default=None, help='forcefully set best'\n",
    "LOAD_LATEST = False  # true or false\n",
    "\n",
    "CHECKPOINT = None        # type=str, default=None\n",
    "MODE = 'train'           # type=str, one of 'train', 'play', 'inspect'\n",
    "GAME = \"LunarLander-v2\"  # type=str\n",
    "CLIP = True              # clipping the delta between -1 and 1 or not\n",
    "SKIP_ACTION = 20         # type=int, default=1, help='skipping actions'\n",
    "RECORD = True            # type=bool\n",
    "INSPECT = False          # type=bool\n",
    "SEED = 42                # type=int\n",
    "\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "logger = logging.getLogger('DQN')\n",
    "logger.setLevel(logging.DEBUG)\n",
    "formatter = logging.Formatter('%(message)s')\n",
    "\n",
    "file_handler = logging.FileHandler(f\"dqn_{MODEL}.log\")\n",
    "file_handler.setFormatter(formatter)\n",
    "logger.addHandler(file_handler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andrew/anaconda3/envs/move_37/lib/python3.6/site-packages/numpy/core/fromnumeric.py:2920: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/home/andrew/anaconda3/envs/move_37/lib/python3.6/site-packages/numpy/core/_methods.py:85: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/home/andrew/anaconda3/envs/move_37/lib/python3.6/site-packages/numpy/core/fromnumeric.py:2920: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/home/andrew/anaconda3/envs/move_37/lib/python3.6/site-packages/numpy/core/_methods.py:85: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/home/andrew/anaconda3/envs/move_37/lib/python3.6/site-packages/ipykernel/__main__.py:212: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "/home/andrew/anaconda3/envs/move_37/lib/python3.6/site-packages/ipykernel/__main__.py:218: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "/home/andrew/anaconda3/envs/move_37/lib/python3.6/site-packages/ipykernel/__main__.py:212: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "/home/andrew/anaconda3/envs/move_37/lib/python3.6/site-packages/ipykernel/__main__.py:218: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Research"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Voice search my browsing history\n",
    "- With a plugin enabled, each site you visit can be searched at a later date using your voice to identify \n",
    "- when you visited it, what the page was about, etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.get_device_name(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.device_ctx_manager(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Big Questions\n",
    "\n",
    "1.  Are we copying data from cpu tensors to gpu tensors in our Model class?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resources\n",
    "\n",
    "- [Github 1](https://github.com/etendue/move37/blob/master/dqn_box2d.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:move_37]",
   "language": "python",
   "name": "conda-env-move_37-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
