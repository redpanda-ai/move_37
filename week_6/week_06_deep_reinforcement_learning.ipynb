{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Deep-RL-for-Database-Optimization\" data-toc-modified-id=\"Deep-RL-for-Database-Optimization-1\">Deep RL for Database Optimization</a></span><ul class=\"toc-item\"><li><span><a href=\"#Video-Description\" data-toc-modified-id=\"Video-Description-1.1\">Video Description</a></span></li><li><span><a href=\"#Notes\" data-toc-modified-id=\"Notes-1.2\">Notes</a></span></li><li><span><a href=\"#Take-Aways\" data-toc-modified-id=\"Take-Aways-1.3\">Take Aways</a></span></li><li><span><a href=\"#Learning-Resources\" data-toc-modified-id=\"Learning-Resources-1.4\">Learning Resources</a></span></li></ul></li><li><span><a href=\"#Deep-Q-Learning-Pong-Tutorial\" data-toc-modified-id=\"Deep-Q-Learning-Pong-Tutorial-2\">Deep Q Learning Pong Tutorial</a></span><ul class=\"toc-item\"><li><span><a href=\"#Notes\" data-toc-modified-id=\"Notes-2.1\">Notes</a></span></li><li><span><a href=\"#Learning-Resources\" data-toc-modified-id=\"Learning-Resources-2.2\">Learning Resources</a></span></li></ul></li><li><span><a href=\"#Prioritized-Experience-Replay-(PER)\" data-toc-modified-id=\"Prioritized-Experience-Replay-(PER)-3\">Prioritized Experience Replay (PER)</a></span><ul class=\"toc-item\"><li><span><a href=\"#Theory\" data-toc-modified-id=\"Theory-3.1\">Theory</a></span></li><li><span><a href=\"#Priority-$p_t$\" data-toc-modified-id=\"Priority-$p_t$-3.2\">Priority $p_t$</a></span></li><li><span><a href=\"#Importance-Sampling-Weights-(IS)\" data-toc-modified-id=\"Importance-Sampling-Weights-(IS)-3.3\">Importance Sampling Weights (IS)</a></span></li><li><span><a href=\"#Google-DeepMind-Paper\" data-toc-modified-id=\"Google-DeepMind-Paper-3.4\">Google DeepMind Paper</a></span></li><li><span><a href=\"#Implementation\" data-toc-modified-id=\"Implementation-3.5\">Implementation</a></span></li><li><span><a href=\"#More-Papers-(Intermediate)\" data-toc-modified-id=\"More-Papers-(Intermediate)-3.6\">More Papers (Intermediate)</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep RL for Database Optimization\n",
    "\n",
    "### Video Description\n",
    "\n",
    "We can use deep reinforcement learning to opitimize a SQL database, and in this video we'll optimize the ordering of a series of SQL queries such that it involves the minimum possible memory/computation footprint.  Deep RL involves using a neural network to approximate reinforcement learning functions, like the Q (quality) function.  After we frame our database as a Markov Decision Process, I'll use Python to build a Deep Q Network to optimize SQL queries.  Enjoy!\n",
    "\n",
    "### Notes\n",
    "\n",
    "SQL:\n",
    "\n",
    "$ SQL\\ Statement \\rightarrow Parsing \\rightarrow (Parse\\ Tree) \\rightarrow Binding \\rightarrow (Algebrized\\ Tree) \\rightarrow Query\\ Optimization \\rightarrow (Execution\\ Plan) \\rightarrow Query\\ Execution \\rightarrow Query\\ Results $\n",
    "\n",
    "### Take Aways\n",
    "\n",
    "- Deep Reinforcement Learning involves using a Neural Network to Approximate Reinforcement Learning Functions like the Q Function\n",
    "- We can assess the quality of Q or State Action Pairs by computing a Q Table\n",
    "- Q Learning involves approximating the relationship between State Action Pairs and Q Values in this table using Neural Networks\n",
    "\n",
    "### Learning Resources\n",
    "- [Youtube Video](https://www.youtube.com/watch?v=Rw3ewEXOKC8)\n",
    "- [Code Link: SQL Database Optimization](https://github.com/llSourcell/SQL_Database_Optimization)\n",
    "- [Irselab: SQL Query Optimization Meets Deep Reinforcement Learning](https://rise.cs.berkeley.edu/blog/sql-query-optimization-meets-deep-reinforcement-learning/)\n",
    "- [MLDB: Machine Learning Database](https://mldb.ai/)\n",
    "- [Microsoft: Machine Learning Services in SQL Server 2017](https://docs.microsoft.com/en-us/sql/advanced-analytics/what-is-sql-server-machine-learning?view=sql-server-2017)\n",
    "- [Towards Data Science: Machine Learning in your Database](https://towardsdatascience.com/machine-learning-in-your-database-the-case-for-and-against-bigquery-ml-4f2309282fda)\n",
    "- [Quora: Which database is best for machine learning](https://www.quora.com/Which-database-is-best-for-machine-learning)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Q Learning Pong Tutorial\n",
    "\n",
    "**Video Description**\n",
    "\n",
    "Learn how to build an AI that plays Pong like a boss, with Deep Q Learning.  Discover how neural networks can learn to play challenging video games at superhuman levels by looking at raw pixels.\n",
    "\n",
    "### Notes\n",
    "\n",
    "**How to play Pong like a boss with Deep Q Learning**\n",
    "\n",
    "- If we can write code which masters complex video games we can use that same code to master complex real-life problems\n",
    "\n",
    "**Taking Q Deep**\n",
    "\n",
    "- In vanilla Q Learning, w are storing a huge lookup table whose size is the number of possible states times the number of possible actions\n",
    "- **PROBLEM**: possible game states are astronomically large!\n",
    "- Bellman Equation remains the same\n",
    "- **SOLUTION**: replace the Q lookup table with a neural network, which approximates a function for Q of state and action\n",
    "- **BEFORE**: we use the most recent value calculation and learning rate to update our Q table\n",
    "- **NOW**: Update the Q Network with stochastic gradient descent (SGD) and backpropogation\n",
    "\n",
    "**Over-simplified Q Learning Algorithm**\n",
    "\n",
    "1.  Initialize $ Q_{s,a} $ randomly\n",
    "2.  Interact with the environment to obtain $ (s, a, r, s') $\n",
    "3.  Calculate loss: $ L\\ =\\ (Q_{s,a} - r)^2 $\n",
    "    otherwise: $ L\\ = \\ (Q_{s,a}\\ -\\ (r\\ + \\gamma max_a [Q_{s',a'}]))^2 $\n",
    "4.  Update $ Q_{s,a} $ using SGD, minimizing the loss function\n",
    "5.  Repeat steps 2 - 4 until converged\n",
    "\n",
    "**Explore vs. Exploit**\n",
    "\n",
    "- To reach an optimal policy, we need to balance exploration with exploitation\n",
    "- Continue to use Epsilon Greedy mthod\n",
    "- Start with $ \\epsilon\\ =\\ 1 $ , taking all random actions\n",
    "- Gradually taper to lower value over a fixed number of game frames\n",
    "\n",
    "**Replay Buffer**\n",
    "\n",
    "- SGD optimization requires Independent and Identically Distributed training data\n",
    "- Our state transitions are highly correlated\n",
    "- Store a long list of $ (s, a, r, s') $ transitions\n",
    "- Randomly sample batchs to train on from the buffer\n",
    "- New data kicks off old data\n",
    "- By randomly sampling from a long list, it breaks the correlation that comes from sampling values right next to each other and allows the model to converge\n",
    "\n",
    "**Target Network**\n",
    "\n",
    "- Loss function, $ L\\ =\\ (Q_{s,a}\\ -\\ (r\\ +\\ \\gamma max_a [Q_{s',a'}]))^2 $\n",
    "- We're updating $ Q_{s,a} $ and $ Q_{s',a'} $ in the same step\n",
    "- We'll store them in a different network (target network)\n",
    "- Copy the weights from the main to target at fixed intervals\n",
    "\n",
    "**Predicting Motion**\n",
    "\n",
    "- Markov Property: The Past Doesn't Matter Baby!\n",
    "- But when there's motion, it does matter\n",
    "- Solution: Stack several recent frames together as input\n",
    "\n",
    "**Network Architecture**\n",
    "\n",
    "- How do we take a large number of pixels from the screen and pick out which objects are important to achieving results in a video game?  The same neural network used in cutting edge image recognition is perfect for applying Reinforcement Learning to screen pixels\n",
    "- Use 3 layers of convolutions with each one passing through a ReLU activation\n",
    "- Input\n",
    "- Conv2d -> ReLU (x3)\n",
    "- Fully connected (512) -> ReLU\n",
    "- Actions: Output layer spits out the values of each action\n",
    "\n",
    "**Deep Q Network Algorithm**\n",
    "\n",
    "1.  Initialize $ Q_{s,a} $ and Q^(s,a) (target network) with random weights\n",
    "2.  With probability $ \\epsilon $, select random action $ a $, otherwise $ a = max_a(Q_{s,a}) $\n",
    "3.  Execute action $a$ in the game, observe reward $r$, next state $s'$\n",
    "4.  Store transition $ (s, a, r, s') $ in the replay buffer\n",
    "5.  Same a random mini-batch of transitions from the replay buffer\n",
    "6.  For every transition in the buffer, calculate target $y = r$ if episode is over, otherwise $y = r\\ +\\ \\gamma max_a(Q_{s',a'})$ \n",
    "7.  Calculate loss: $L\\ =\\ (Q_{s,a}\\ - y)^2$\n",
    "8.  Update $Q_{s,a}$ using SGD, minimizing the loss function\n",
    "9.  Every N steps copy weights from Q to Q^\n",
    "10.  Repeat from step 2 until converged\n",
    "\n",
    "Deep Learning: use either PyTorch (easier) or Tensorflow (great for production, steeper learning curve)\n",
    "\n",
    "**Simple then Expand**\n",
    "\n",
    "- Reinforcement Learning started out with simple applications of the Bellman Equation\n",
    "- Gradually involved enhancements and workarounds when that performed poorly on tasks\n",
    "- Basic implementation of Q Learning can only handle fairly simple tasks so we're going to start out with Pong\n",
    "- Learning Deep Q enhancements, we can try out more complex games like Doom\n",
    "\n",
    "**Wrappers**\n",
    "\n",
    "- In OpenAI Gym, Wrappers are a layer of code that takes observations (raw pixels from the environment) and processes them before they enter the neural network\n",
    "- A layer of code around OpenAI gym\n",
    "- Transforms observations before passing them to the network\n",
    "- Transforms actions before passing them to the environment\n",
    "\n",
    "**How to run Deep Q Pong**\n",
    "- ```dqn_basic.py --cuda```\n",
    "- ```tensorboard --logdir runs```\n",
    "- took ~2 hours to train on a Nvidia GTX 1080ti GPU\n",
    "\n",
    "### Learning Resources\n",
    "- [Youtube Video: Deep Q Learning Pong Tutorial](https://www.youtube.com/watch?v=pST6caY3mu8)\n",
    "- [Code Link: DQN Pong](https://github.com/colinskow/move37/tree/master/dqn)\n",
    "- [Siraj: Image Recognition Tutorial](https://www.youtube.com/watch?v=cAICT4Al5Ow)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prioritized Experience Replay (PER)\n",
    "\n",
    "This article is mainly citing Thomas Simonini's blog\n",
    "    - Thomas Simonini's blog about Improvements in Deep Q Learning\n",
    "    - Prioritized Experience Replay by Google DeepMind\n",
    "    - SLM Lab@School of AI Github\n",
    "    - OpenAI Github\n",
    "    - Patrick Emami's blog\n",
    "    - Jaromiur's blog about Let's Make a DQN\n",
    "    \n",
    "### Theory\n",
    "\n",
    "- Some experiences may be more important than others for our training, but might occur less frequently\n",
    "- Because we sample the batch uniformly (selecting the experiences randomly) these rich experiences that occur rarely happen and have practically no chance to be selected\n",
    "\n",
    "### Priority $p_t$\n",
    "\n",
    "- We want to take in a priority experience where there is a big difference between our prediction and the TD target, since it means that we have a lot to learn about it\n",
    "- Define Priority $p_t$ as:\n",
    "\n",
    "$$ \\large p_t = |\\delta_{t}| + e $$\n",
    "\n",
    "$|\\delta_{t}|$: Magnitude of our TD error\n",
    "\n",
    "$e$: Constant assures that no experience has 0 probability to be taken\n",
    "\n",
    "- TD Error: $error\\ = |Q(s,a) - T(S)|$ where $T(s)\\ =\\ r + \\gamma Q(s', argmax_aQ(s'a))$\n",
    "\n",
    "### Importance Sampling Weights (IS)\n",
    "\n",
    "- Samples that have high priority are likely to be used for training many times in comparison with low priority experiences (=bias)\n",
    "- Therefore, we will update our weights with only a small portion of experiences that we consider to be really interesting\n",
    "- To correct this bias, we use **importance sampling weights** (IS) that will **adjust the updating by reducing the weights** of the often seen samples\n",
    "\n",
    "$$\\large W_i\\ =\\ (\\frac{1}{N}\\ *\\ \\frac{1}{P(i)})^b $$\n",
    "\n",
    "where\n",
    "- $\\frac{1}{N}$ is the Replay Buffer Size\n",
    "- $P(i)$ is the Sampling Probability\n",
    "- $b$ controls how mus the IS will affect learning\n",
    "- Close to 0 at the beginning of the learning and annealed up to 1 over the duration of the training because **these weights are more important in the end of the learning when our Q values begin to converge**\n",
    "\n",
    "### Google DeepMind Paper\n",
    "\n",
    "Define Priority $p_i$, pick probability of $P(j)$ and update with importance sampling weight $w_i$.\n",
    "\n",
    "\n",
    "![Google DeepMind PER Paper Algorithm 1](imgs/move_37_google_deepmind_per_paper_algorithm1.jpg)\n",
    "\n",
    "<br/>\n",
    "\n",
    "### Implementation\n",
    "\n",
    "- TODO: Add relevant code parts!\n",
    "\n",
    "- Priority Part ([SLM Lab](https://github.com/kengz/SLM-Lab/blob/master/slm_lab/agent/memory/prioritized.py))\n",
    "- Priority Part ([OpenAI](https://github.com/openai/baselines/blob/master/baselines/deepq/replay_buffer.py))\n",
    "- Probability and IS Part ([OpenAI](https://github.com/openai/baselines/blob/master/baselines/deepq/replay_buffer.py))\n",
    "\n",
    "### More Papers (Intermediate)\n",
    "\n",
    "- [Distributed Prioritized Experience Replay ICLR 2018](https://arxiv.org/abs/1803.00933)\n",
    "- [A Deeper Look at Planning as Learning from Replay (Richard Sutton 2015)](http://proceedings.mlr.press/v37/vanseijen15.pdf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:move_37]",
   "language": "python",
   "name": "conda-env-move_37-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
